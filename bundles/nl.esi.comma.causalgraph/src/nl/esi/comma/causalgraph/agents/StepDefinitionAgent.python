import os
import re

from langchain_openai import AzureChatOpenAI
from lark import Lark
from llm4requirements.agents.agent_workflows.merge_agent_workflow import construct_workflow
from llm4requirements.agents.chains.lark.cg_extractor import (
    get_overlay_edges,
    get_overlay_scenario,
    get_overlay_variables,
)
from llm4requirements.agents.chains.lark.cg_serializer import BDDSerializer
from llm4requirements.agents.chains.lark.cg_tree_transformer import transform_overlay_tree_to_bdd
from llm4requirements.utils.data import get_config
from llm4requirements.utils.logging import setup_logging
from llm4requirements.utils.save_md_file import generate_filename, save_file


class MergeAgent:
    """
    MergeAgent class for processing code snippets and generating step definitions.
    This class initializes the LLM and constructs the workflow for merging code snippets.
    """

    def __init__(self, config: dict, source_code_files: list):
        """Initialize the MergeAgent with the provided configuration.

        Parameters:
            config (dict): The configuration dictionary.
        """
        setup_logging()

        AZURE_ENDPOINT = config.get("gpt", "endpoint")
        AZURE_API_KEY = config.get("gpt", "api_key")
        LLM = config.get("gpt", "llm")
        API_VERSION = config.get("azure-api", "version")
        os.environ["AZURE_OPENAI_API_KEY"] = AZURE_API_KEY
        os.environ["AZURE_OPENAI_ENDPOINT"] = AZURE_ENDPOINT

        self.llm = AzureChatOpenAI(model=LLM, api_version=API_VERSION, temperature=1, )

        # Load source code files
        self.source_code = self._load_source_code(source_code_files)

        self.static_inputs = {
            "llm": self.llm,
            "source_code": self.source_code,
        }
        self.compiled_workflow = construct_workflow()

    def _load_source_code(self, source_code_files: list) -> dict[str, str]:
        """Load source code files for reference.

        Args:
            source_code_files (list): List of paths to source code files.

        Returns:
            dict[str, str]: Dictionary mapping filename to file content.
        """
        source_code = {}
        for path in source_code_files:
            try:
                with open(path) as f:
                    source_code[path] = f.read()
            except FileNotFoundError:
                source_code[path] = f"// {path} not found"

        return source_code

    def _build_data_dependency_map(self, edges: list) -> dict:
        """Build a map of data dependencies for each node and test case.

        Args:
            edges (list): List of edge dictionaries from get_overlay_edges

        Returns:
            dict: Mapping of {consumer_node: {test_case: [provider_nodes]}}
        """
        data_dependencies = {}

        # Only process data edges, which contain specific test case dependencies
        for edge in edges:
            if edge["type"] == "data":
                # The semantics: source_node consumes data produced by target_node
                # So n4 -> n3 means n4 needs data from n3
                consumer_node = edge["source"]
                provider_node = edge["target"]
                data_refs = edge["data_refs"]

                if consumer_node not in data_dependencies:
                    data_dependencies[consumer_node] = {}

                # Try to extract test cases from the data references
                # Convert data_refs tree to string to extract test cases
                data_refs_str = str(data_refs)

                # TO DO make this general
                matches = re.findall(r"causal_graph_T(\d+)", data_refs_str)
                for match in matches:
                    test_case = f"T{match}"
                    if test_case not in data_dependencies[consumer_node]:
                        data_dependencies[consumer_node][test_case] = []
                    if provider_node not in data_dependencies[consumer_node][test_case]:
                        data_dependencies[consumer_node][test_case].append(provider_node)

        return data_dependencies

    def invoke(self, code: str, type_code: str) -> str:
        """Invoke the LLM module to process the code snippets in the graph file and generate step definitions for the pieces of code.

        Args:
            code (str): The causal graph file needed to be processed.
            type_code (str): The type of code snippet (e.g., 'plantuml', 'cg').
        Returns:
            str: The generated graph file content where pieces of code are replaced by step definitions.
        """
        overlay_grammar_file_path = "src/llm4requirements/agents/chains/lark/bdd_cg_grammar.lark"
        with open(overlay_grammar_file_path) as read_file:
            overlay_grammar = read_file.read()
        overlay_parser = Lark(overlay_grammar)
        overlay_tree = overlay_parser.parse(code)

        nodes_info = get_overlay_scenario(overlay_tree)
        variables, variable_initial_values = get_overlay_variables(overlay_tree)
        overlay_edges = get_overlay_edges(overlay_tree)
        nodes = list(nodes_info.keys())
        node_step_definitions = {}
        original_to_new_nodes = {}

        data_dependencies = self._build_data_dependency_map(overlay_edges)

        for node, scenario_steps in nodes_info.items():
            # collect which test case are in this node
            current_test_cases = set()
            for step in scenario_steps:
                scenario_id = step.get("scenario_id", "")
                if "causal_graph_T" in scenario_id:
                    parts = scenario_id.split("causal_graph_T")
                    if len(parts) > 1:
                        t_part = parts[1].split("_")[0].split(",")[0].split(" ")[0]
                        current_test_cases.add(f"T{t_part}")

            previous_overlay_steps = {}

            # Get previous step definitions based upon data dependencies
            current_node_dependencies = data_dependencies.get(node, {})

            # Collect original overlay scenario steps based on data dependencies
            for prev_node, prev_scenario_steps in nodes_info.items():
                if prev_node != node:
                    # Check if this previous node is needed based on data dependencies
                    is_needed = False
                    for test_case in current_test_cases:
                        if test_case in current_node_dependencies and prev_node in current_node_dependencies[test_case]:
                            is_needed = True
                            break

                    if is_needed:
                        previous_overlay_steps[prev_node] = prev_scenario_steps

            merge_agent_input = {
                **self.static_inputs,
                "scenario_steps": scenario_steps,
                "overlay_variables": variables,
                "variable_initial_values": variable_initial_values,
                "previous_overlay_steps": previous_overlay_steps,
            }

            result = self.compiled_workflow.invoke(merge_agent_input)

            # add step_type to the node
            step_type = scenario_steps[0]["step_type"]
            for step_def in result["result"]:
                step_def["step_type"] = step_type

            # if the node has splitted make a new node
            if len(result["result"]) > 1:
                node_step_definitions[node] = result["result"][0]
                new_nodes = [node]

                for i in range(1, len(result["result"])):
                    highest_node_str = nodes[-1].split("n")[-1] if "n" in nodes[-1] else nodes[-1].split("N")[-1]
                    highest_node = int(highest_node_str)
                    new_node = f"n{highest_node + i}"
                    node_step_definitions[new_node] = result["result"][i]
                    new_nodes.append(new_node)
                    nodes.append(new_node)

                original_to_new_nodes[node] = new_nodes
            else:
                node_step_definitions[node] = result["result"][0]
                original_to_new_nodes[node] = [node]

        bdd_tree = transform_overlay_tree_to_bdd(overlay_tree, node_step_definitions, original_to_new_nodes, variables)
        bdd_serializer = BDDSerializer()
        cg_source = bdd_serializer.transform(bdd_tree)

        filename = generate_filename(type_file="bdducg")
        save_file(cg_source, filename)
        return cg_source


if __name__ == "__main__":
    config = get_config()
    source_code_files = ["source_code/test_acquisition.cpp"]
    bdd_agent = MergeAgent(config, source_code_files)

    # file = "cg_files/overlay_cg.cg"
    file = "cg_files/vending_machine_unified_causal_graph_test2_arithmetic.cg"
    # file = "cg_files/vending_machine_unified_causal_graph.cg"
    # file = "cg_files/vending_machine_unified_causal_graph_test_node_split.cg"
    file = "cg_files/PhilipsDemoTestAcquisition_dsaLateral__PhilipsDemoTestAcquisition_rcmImageFreezeFrontal__PhilipsDemoTestAcquisition_rcmImageFreezeLateral.ucg"
    file = "cg_files/PhilipsDemo_dsaLateral__PhilipsDemo_rcmImageFreezeFrontal_270825.ucg"
    # file = "cg_files/vending_machine_unified_causal_graph_less_T.cg"
    #file = "cg_files/TestAcquisition_rcmImageFreezeFrontal__TestAcquisition_rcmImageFreezeLateral.ucg"
    file = "cg_files/TestAcquisition_rcmImageFreezeFrontal__TestAcquisition_rcmImageFreezeLateral 2.ucg"
    with open(file) as read_file:
        cg_code = read_file.read()

    type_code = file.split(".")[-1]
    bdd_agent.invoke(cg_code, type_code)
